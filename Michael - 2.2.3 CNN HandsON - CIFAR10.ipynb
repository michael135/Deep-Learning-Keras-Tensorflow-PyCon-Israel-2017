{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "In this second exercise-notebook we will play with Convolutional Neural Network (CNN). \n",
    "\n",
    "As you should have seen, a CNN is a feed-forward neural network tipically composed of Convolutional, MaxPooling and Dense layers. \n",
    "\n",
    "If the task implemented by the CNN is a classification task, the last Dense layer should use the **Softmax** activation, and the loss should be the **categorical crossentropy**.\n",
    "\n",
    "Reference: [https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Topology Model\n",
    "\n",
    "A simple CNN, with one input branch and one output branch can be defined using a [Sequential](http://keras.io/models/#sequential) model and stacking together all its layers. \n",
    "\n",
    "In this exercise we want to build a (_quite shallow_) network which contains two \n",
    "[Convolution, Convolution, MaxPooling] stages, and two Dense layers.\n",
    "\n",
    "To test a different optimizer, we will use [AdaDelta](http://keras.io/optimizers/), which is a bit more complex than the simple Vanilla SGD with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.normalization import BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_shape = (3, 32, 32)\n",
    "nb_classes = 10\n",
    "\n",
    "\n",
    "nb_epoch = 10 # kept very low! Please increase if you have GPU\n",
    "\n",
    "batch_size = 64\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "nb_pool = 2\n",
    "# convolution kernel size\n",
    "nb_conv = 3\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "## [conv@32x3x3+relu]x2 --> MaxPool@2x2 --> DropOut@0.25 -->\n",
    "## [conv@64x3x3+relu]x2 --> MaxPool@2x2 --> DropOut@0.25 -->\n",
    "## Flatten--> FC@512+relu --> DropOut@0.5 --> FC@nb_classes+SoftMax\n",
    "## NOTE: each couple of Conv filters must have `border_mode=\"same\"` and `\"valid\"`, respectively\n",
    "\n",
    "## your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = 32, 32\n",
    "shape_ord = (img_rows, img_cols, 3)\n",
    "nb_filters_small = 4\n",
    "nb_pool_small = 4\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(nb_filters_small, (nb_conv, nb_conv), padding='valid', \n",
    "                 input_shape=shape_ord))  # note: the very first layer **must** always specify the input_shape\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool_small, nb_pool_small)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/sol_223.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_filters_small = 4\n",
    "nb_pool_small = 4\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(nb_filters_small, (32, 32), padding='valid', \n",
    "                 input_shape=shape_ord))  # note: the very first layer **must** always specify the input_shape\n",
    "\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool_small, nb_pool_small)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(nb_filters_small, (64, 64), padding='valid', \n",
    "                 input_shape=shape_ord))  # note: the very first layer **must** always specify the input_shape\n",
    "\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool_small, nb_pool_small)))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding layer shapes\n",
    "\n",
    "An important feature of Keras layers is that each of them has an `input_shape` attribute, which you can use to visualize the shape of the input tensor, and an `output_shape` attribute, for inspecting the shape of the output tensor.\n",
    "\n",
    "As we can see, the input shape of the first convolutional layer corresponds to the `input_shape` attribute (which must be specified by the user). \n",
    "\n",
    "In this case, it is a `32x32` image with three color channels. \n",
    "\n",
    "Since this convolutional layer has the `border_mode` set to `same`, its output width and height will remain the same, and the number of output channel will be equal to the number of filters learned by the layer, 16. \n",
    "\n",
    "The following convolutional layers, instead, have the default `border_mode`, and therefore reduce width and height by $(k-1)$, where $k$ is the size of the kernel. \n",
    "\n",
    "MaxPooling layers, instead, reduce width and height of the input tensor, but keep the same number of channels. Activation layers, of course, don't change the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 \t(None, 32, 32, 3) \t(None, 30, 30, 4)\n",
      "Layer 1 \t(None, 30, 30, 4) \t(None, 30, 30, 4)\n",
      "Layer 2 \t(None, 30, 30, 4) \t(None, 30, 30, 4)\n",
      "Layer 3 \t(None, 30, 30, 4) \t(None, 7, 7, 4)\n",
      "Layer 4 \t(None, 7, 7, 4) \t(None, 196)\n",
      "Layer 5 \t(None, 196) \t(None, 10)\n",
      "Layer 6 \t(None, 10) \t(None, 10)\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    print \"Layer\", i, \"\\t\", layer.input_shape, \"\\t\", layer.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding weights shape\n",
    "In the same way, we can visualize the shape of the weights learned by each layer. In particular, Keras lets you inspect weights by using the `get_weights` method of a layer object. This will return a list with two elements, the first one being the weight tensor and the second one being the bias vector.\n",
    "\n",
    "Of course, MaxPooling layer don't have any weight tensor, since they don't have learnable parameters. Convolutional layers, instead, learn a $(n_o, n_i, k, k)$ or **$(k, k, n_i, n_o)$** weight tensor, where $k$ is the size of the kernel, $n_i$ is the number of channels of the input tensor, and $n_o$ is the number of filters to be learned. For each of the $n_o$ filters, a bias is also learned. Dense layers learn a $(n_i, n_o)$ weight tensor, where $n_o$ is the output size and $n_i$ is the input size of the layer. Each of the $n_o$ neurons also has a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 \t (3, 3, 5, 4) \t (4,)\n",
      "Layer 1 \t (4,) \t (4,)\n",
      "Layer 5 \t (196, 10) \t (10,)\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    if len(layer.get_weights()) > 0:\n",
    "        print(\"Layer\", i, \"\\t\", layer.get_weights()[0].shape, \"\\t\", layer.get_weights()[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "We will train our network on the **CIFAR10** [dataset](https://www.cs.toronto.edu/~kriz/cifar.html), which contains `50,000` 32x32 color training images, labeled over 10 categories, and 10,000 test images. \n",
    "\n",
    "As this dataset is also included in Keras datasets, we just ask the `keras.datasets` module for the dataset.\n",
    "\n",
    "Training and test images are normalized to lie in the $\\left[0,1\\right]$ interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 14s - loss: 2.0459 - acc: 0.2716 - val_loss: 1.8912 - val_acc: 0.3144\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 15s - loss: 1.8171 - acc: 0.3512 - val_loss: 1.7448 - val_acc: 0.3813\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 16s - loss: 1.7001 - acc: 0.3941 - val_loss: 1.9825 - val_acc: 0.3446\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 18s - loss: 1.6213 - acc: 0.4221 - val_loss: 1.6490 - val_acc: 0.4209\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 18s - loss: 1.5641 - acc: 0.4433 - val_loss: 1.7495 - val_acc: 0.4012\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 19s - loss: 1.5251 - acc: 0.4567 - val_loss: 1.6340 - val_acc: 0.4336\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 19s - loss: 1.5006 - acc: 0.4684 - val_loss: 1.6388 - val_acc: 0.4191\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 19s - loss: 1.4807 - acc: 0.4755 - val_loss: 1.6252 - val_acc: 0.4216\n",
      "Epoch 9/10\n",
      "49856/50000 [============================>.] - ETA: 0s - loss: 1.4660 - acc: 0.4824"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train, batch_size=batch_size, \n",
    "                 epochs=nb_epoch, verbose=1, \n",
    "                 validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/EIkioJfiET5P500nAa6j2JdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x124f48c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "imgplot = plt.imshow(X_train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the risk of overfitting, we also apply some image transformation, like rotations, shifts and flips. All these can be easily implemented using the Keras [Image Data Generator](http://keras.io/preprocessing/image/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warning: The following cells may be computational Intensive...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "generated_images = ImageDataGenerator(\n",
    "    featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "generated_images.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start training. \n",
    "\n",
    "At each iteration, a batch of 500 images is requested to the `ImageDataGenerator` object, and then fed to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3, 32, 32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = generated_images.flow(X_train, Y_train, batch_size=500, shuffle=True)\n",
    "X_batch, Y_batch = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 3, 32, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import generic_utils\n",
    "\n",
    "n_epochs = 2\n",
    "for e in range(n_epochs):\n",
    "    print('Epoch', e)\n",
    "    print('Training...')\n",
    "    progbar = generic_utils.Progbar(X_train.shape[0])\n",
    "    \n",
    "    for X_batch, Y_batch in generated_images.flow(X_train, Y_train, batch_size=500, shuffle=True):\n",
    "        loss = model.train_on_batch(X_batch, Y_batch)\n",
    "        progbar.add(X_batch.shape[0], values=[('train loss', loss[0])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
